// ram_monitor.c
#include "vmlinux.h"
#include <bpf/bpf_helpers.h>
#include <bpf/bpf_tracing.h>

#ifndef PAGE_SIZE
#define PAGE_SIZE 4096ULL
#endif

#ifndef TASK_COMM_LEN
#define TASK_COMM_LEN 16
#endif

struct mem_key
{
    u32 pid;
    char comm[TASK_COMM_LEN];
};

struct mem_stats
{
    u64 alloc_bytes;
    u64 free_bytes;
};

struct
{
    __uint(type, BPF_MAP_TYPE_HASH);
    __uint(max_entries, 16384);
    __type(key, struct mem_key);
    __type(value, struct mem_stats);
} ram_usage SEC(".maps");

static __always_inline u64 order_to_bytes(int order)
{
    if (order <= 0)
    {
        return PAGE_SIZE;
    }
    return ((u64)1 << order) * PAGE_SIZE;
}

static __always_inline void fill_key(struct mem_key *k)
{
    k->pid = bpf_get_current_pid_tgid() >> 32;
    bpf_get_current_comm(&k->comm, sizeof(k->comm));
}

SEC("tracepoint/kmem/mm_page_alloc")
int trace_alloc(struct trace_event_raw_mm_page_alloc *ctx)
{
    struct mem_key key = {};
    fill_key(&key);

    int order = ctx->order;
    u64 size = order_to_bytes(order);

    struct mem_stats *val = bpf_map_lookup_elem(&ram_usage, &key);
    if (val)
    {
        __sync_fetch_and_add(&val->alloc_bytes, size);
    }
    else
    {
        struct mem_stats init = {.alloc_bytes = size, .free_bytes = 0};
        bpf_map_update_elem(&ram_usage, &key, &init, BPF_ANY);
    }

    return 0;
}

SEC("tracepoint/kmem/mm_page_free")
int trace_free(struct trace_event_raw_mm_page_free *ctx)
{
    struct mem_key key = {};
    fill_key(&key);

    int order = ctx->order;
    u64 size = order_to_bytes(order);

    struct mem_stats *val = bpf_map_lookup_elem(&ram_usage, &key);
    if (val)
    {
        __sync_fetch_and_add(&val->free_bytes, size);
    }
    else
    {
        struct mem_stats init = {.alloc_bytes = 0, .free_bytes = size};
        bpf_map_update_elem(&ram_usage, &key, &init, BPF_ANY);
    }

    return 0;
}

char LICENSE[] SEC("license") = "GPL";
